name: NGramTokenizer
uid: '@azure/search-documents.NGramTokenizer|preview'
package: '@azure/search-documents|preview'
summary: >-
  Tokenizes the input into n-grams of the given size(s). This tokenizer is
  implemented using Apache Lucene.
fullName: NGramTokenizer
remarks: ''
isPreview: false
isDeprecated: false
syntax: >-
  type NGramTokenizer = BaseLexicalTokenizer & { maxGram: number, minGram:
  number, odatatype: #Microsoft.Azure.Search.NGramTokenizer, tokenChars: Object
  }
