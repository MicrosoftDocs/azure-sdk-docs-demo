name: KnownTokenFilterName
uid: '@azure/search-documents.KnownTokenFilterName|latest-stable'
package: '@azure/search-documents|latest-stable'
summary: >-
  Known values of <xref:@azure/search-documents.TokenFilterName> that the
  service accepts.
fullName: KnownTokenFilterName
remarks: ''
isPreview: false
isDeprecated: false
fields:
  - name: Apostrophe
    uid: '@azure/search-documents.KnownTokenFilterName.Apostrophe|latest-stable'
    package: '@azure/search-documents|latest-stable'
    summary: >-
      Strips all characters after an apostrophe (including the apostrophe
      itself). See
      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/tr/ApostropheFilter.html
  - name: ArabicNormalization
    uid: >-
      @azure/search-documents.KnownTokenFilterName.ArabicNormalization|latest-stable
    package: '@azure/search-documents|latest-stable'
    summary: >-
      A token filter that applies the Arabic normalizer to normalize the
      orthography. See
      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/ar/ArabicNormalizationFilter.html
  - name: AsciiFolding
    uid: '@azure/search-documents.KnownTokenFilterName.AsciiFolding|latest-stable'
    package: '@azure/search-documents|latest-stable'
    summary: >-
      Converts alphabetic, numeric, and symbolic Unicode characters which are
      not in the first 127 ASCII characters (the "Basic Latin" Unicode block)
      into their ASCII equivalents, if such equivalents exist. See
      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/miscellaneous/ASCIIFoldingFilter.html
  - name: CjkBigram
    uid: '@azure/search-documents.KnownTokenFilterName.CjkBigram|latest-stable'
    package: '@azure/search-documents|latest-stable'
    summary: >-
      Forms bigrams of CJK terms that are generated from the standard tokenizer.
      See
      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/cjk/CJKBigramFilter.html
  - name: CjkWidth
    uid: '@azure/search-documents.KnownTokenFilterName.CjkWidth|latest-stable'
    package: '@azure/search-documents|latest-stable'
    summary: >-
      Normalizes CJK width differences. Folds fullwidth ASCII variants into the
      equivalent basic Latin, and half-width Katakana variants into the
      equivalent Kana. See
      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/cjk/CJKWidthFilter.html
  - name: Classic
    uid: '@azure/search-documents.KnownTokenFilterName.Classic|latest-stable'
    package: '@azure/search-documents|latest-stable'
    summary: >-
      Removes English possessives, and dots from acronyms. See
      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/standard/ClassicFilter.html
  - name: CommonGram
    uid: '@azure/search-documents.KnownTokenFilterName.CommonGram|latest-stable'
    package: '@azure/search-documents|latest-stable'
    summary: >-
      Construct bigrams for frequently occurring terms while indexing. Single
      terms are still indexed too, with bigrams overlaid. See
      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/commongrams/CommonGramsFilter.html
  - name: EdgeNGram
    uid: '@azure/search-documents.KnownTokenFilterName.EdgeNGram|latest-stable'
    package: '@azure/search-documents|latest-stable'
    summary: >-
      Generates n-grams of the given size(s) starting from the front or the back
      of an input token. See
      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/ngram/EdgeNGramTokenFilter.html
  - name: Elision
    uid: '@azure/search-documents.KnownTokenFilterName.Elision|latest-stable'
    package: '@azure/search-documents|latest-stable'
    summary: >-
      Removes elisions. For example, "l'avion" (the plane) will be converted to
      "avion" (plane). See
      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/util/ElisionFilter.html
  - name: GermanNormalization
    uid: >-
      @azure/search-documents.KnownTokenFilterName.GermanNormalization|latest-stable
    package: '@azure/search-documents|latest-stable'
    summary: >-
      Normalizes German characters according to the heuristics of the German2
      snowball algorithm. See
      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/de/GermanNormalizationFilter.html
  - name: HindiNormalization
    uid: >-
      @azure/search-documents.KnownTokenFilterName.HindiNormalization|latest-stable
    package: '@azure/search-documents|latest-stable'
    summary: >-
      Normalizes text in Hindi to remove some differences in spelling
      variations. See
      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/hi/HindiNormalizationFilter.html
  - name: IndicNormalization
    uid: >-
      @azure/search-documents.KnownTokenFilterName.IndicNormalization|latest-stable
    package: '@azure/search-documents|latest-stable'
    summary: >-
      Normalizes the Unicode representation of text in Indian languages. See
      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/in/IndicNormalizationFilter.html
  - name: KStem
    uid: '@azure/search-documents.KnownTokenFilterName.KStem|latest-stable'
    package: '@azure/search-documents|latest-stable'
    summary: >-
      A high-performance kstem filter for English. See
      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/en/KStemFilter.html
  - name: KeywordRepeat
    uid: '@azure/search-documents.KnownTokenFilterName.KeywordRepeat|latest-stable'
    package: '@azure/search-documents|latest-stable'
    summary: >-
      Emits each incoming token twice, once as keyword and once as non-keyword.
      See
      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/miscellaneous/KeywordRepeatFilter.html
  - name: Length
    uid: '@azure/search-documents.KnownTokenFilterName.Length|latest-stable'
    package: '@azure/search-documents|latest-stable'
    summary: >-
      Removes words that are too long or too short. See
      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/miscellaneous/LengthFilter.html
  - name: Limit
    uid: '@azure/search-documents.KnownTokenFilterName.Limit|latest-stable'
    package: '@azure/search-documents|latest-stable'
    summary: >-
      Limits the number of tokens while indexing. See
      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/miscellaneous/LimitTokenCountFilter.html
  - name: Lowercase
    uid: '@azure/search-documents.KnownTokenFilterName.Lowercase|latest-stable'
    package: '@azure/search-documents|latest-stable'
    summary: >-
      Normalizes token text to lower case. See
      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/core/LowerCaseFilter.htm
  - name: NGram
    uid: '@azure/search-documents.KnownTokenFilterName.NGram|latest-stable'
    package: '@azure/search-documents|latest-stable'
    summary: >-
      Generates n-grams of the given size(s). See
      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/ngram/NGramTokenFilter.html
  - name: PersianNormalization
    uid: >-
      @azure/search-documents.KnownTokenFilterName.PersianNormalization|latest-stable
    package: '@azure/search-documents|latest-stable'
    summary: >-
      Applies normalization for Persian. See
      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/fa/PersianNormalizationFilter.html
  - name: Phonetic
    uid: '@azure/search-documents.KnownTokenFilterName.Phonetic|latest-stable'
    package: '@azure/search-documents|latest-stable'
    summary: >-
      Create tokens for phonetic matches. See
      https://lucene.apache.org/core/4_10_3/analyzers-phonetic/org/apache/lucene/analysis/phonetic/package-tree.html
  - name: PorterStem
    uid: '@azure/search-documents.KnownTokenFilterName.PorterStem|latest-stable'
    package: '@azure/search-documents|latest-stable'
    summary: >-
      Uses the Porter stemming algorithm to transform the token stream. See
      http://tartarus.org/~martin/PorterStemmer
  - name: Reverse
    uid: '@azure/search-documents.KnownTokenFilterName.Reverse|latest-stable'
    package: '@azure/search-documents|latest-stable'
    summary: >-
      Reverses the token string. See
      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/reverse/ReverseStringFilter.html
  - name: ScandinavianFoldingNormalization
    uid: >-
      @azure/search-documents.KnownTokenFilterName.ScandinavianFoldingNormalization|latest-stable
    package: '@azure/search-documents|latest-stable'
    summary: >-
      Folds Scandinavian characters åÅäæÄÆ-&gt;a and öÖøØ-&gt;o. It also
      discriminates against use of double vowels aa, ae, ao, oe and oo, leaving
      just the first one. See
      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/miscellaneous/ScandinavianFoldingFilter.html
  - name: ScandinavianNormalization
    uid: >-
      @azure/search-documents.KnownTokenFilterName.ScandinavianNormalization|latest-stable
    package: '@azure/search-documents|latest-stable'
    summary: >-
      Normalizes use of the interchangeable Scandinavian characters. See
      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/miscellaneous/ScandinavianNormalizationFilter.html
  - name: Shingle
    uid: '@azure/search-documents.KnownTokenFilterName.Shingle|latest-stable'
    package: '@azure/search-documents|latest-stable'
    summary: >-
      Creates combinations of tokens as a single token. See
      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/shingle/ShingleFilter.html
  - name: Snowball
    uid: '@azure/search-documents.KnownTokenFilterName.Snowball|latest-stable'
    package: '@azure/search-documents|latest-stable'
    summary: >-
      A filter that stems words using a Snowball-generated stemmer. See
      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/snowball/SnowballFilter.html
  - name: SoraniNormalization
    uid: >-
      @azure/search-documents.KnownTokenFilterName.SoraniNormalization|latest-stable
    package: '@azure/search-documents|latest-stable'
    summary: >-
      Normalizes the Unicode representation of Sorani text. See
      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/ckb/SoraniNormalizationFilter.html
  - name: Stemmer
    uid: '@azure/search-documents.KnownTokenFilterName.Stemmer|latest-stable'
    package: '@azure/search-documents|latest-stable'
    summary: >-
      Language specific stemming filter. See
      https://docs.microsoft.com/rest/api/searchservice/Custom-analyzers-in-Azure-Search#TokenFilters
  - name: Stopwords
    uid: '@azure/search-documents.KnownTokenFilterName.Stopwords|latest-stable'
    package: '@azure/search-documents|latest-stable'
    summary: >-
      Removes stop words from a token stream. See
      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/core/StopFilter.html
  - name: Trim
    uid: '@azure/search-documents.KnownTokenFilterName.Trim|latest-stable'
    package: '@azure/search-documents|latest-stable'
    summary: >-
      Trims leading and trailing whitespace from tokens. See
      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/miscellaneous/TrimFilter.html
  - name: Truncate
    uid: '@azure/search-documents.KnownTokenFilterName.Truncate|latest-stable'
    package: '@azure/search-documents|latest-stable'
    summary: >-
      Truncates the terms to a specific length. See
      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/miscellaneous/TruncateTokenFilter.html
  - name: Unique
    uid: '@azure/search-documents.KnownTokenFilterName.Unique|latest-stable'
    package: '@azure/search-documents|latest-stable'
    summary: >-
      Filters out tokens with same text as the previous token. See
      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/miscellaneous/RemoveDuplicatesTokenFilter.html
  - name: Uppercase
    uid: '@azure/search-documents.KnownTokenFilterName.Uppercase|latest-stable'
    package: '@azure/search-documents|latest-stable'
    summary: >-
      Normalizes token text to upper case. See
      http://lucene.apache.org/core/4_10_3/analyzers-common/org/apache/lucene/analysis/core/UpperCaseFilter.html
  - name: WordDelimiter
    uid: '@azure/search-documents.KnownTokenFilterName.WordDelimiter|latest-stable'
    package: '@azure/search-documents|latest-stable'
    summary: >-
      Splits words into subwords and performs optional transformations on
      subword groups.
